{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos librerías"
      ],
      "metadata": {
        "id": "XfSzE9WT8ke7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qj355oal7Yv2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import sklearn as sk\n",
        "from sklearn import model_selection\n",
        "from sklearn import ensemble\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import imblearn as im\n",
        "from imblearn import under_sampling\n",
        "from imblearn import over_sampling\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función para calcular las métricas accuracy, precision, recall y f1 tomado del colab de la materia."
      ],
      "metadata": {
        "id": "xbquDhkP8i80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_precision_recall(X, y):\n",
        "    X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X, y, test_size=0.5, shuffle=True, stratify=y, random_state=42)\n",
        "\n",
        "    clf = sk.ensemble.RandomForestClassifier(max_features= 'log2',min_samples_split = 20, min_samples_leaf = 10, n_estimators=n_estimators, max_depth=max_depth, n_jobs=-1, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    print(\"accuracy:\", sk.metrics.accuracy_score(y_test, y_pred))\n",
        "    print(\"precision:\", sk.metrics.precision_score(y_test, y_pred))\n",
        "    print(\"recall:\", sk.metrics.recall_score(y_test, y_pred))\n",
        "    print(\"f1:\",  sk.metrics.f1_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "_j0YQ13087w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta función calcula cuanto gustó/no gustó un género (si género es lo que se ingresa en el campo_group_by) a un usuario tal como lo explicaron los docentes en clase\n",
        "en campo_id va 'id_lector' y en campo_label 'label'.\n",
        "esta versión calcula una proporción \"ponderada\"\n",
        "proporción = (likes-dislikes) * (likes+dislikes) / total de opiniones del usuario"
      ],
      "metadata": {
        "id": "LGy0v6IB8_7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_columna_prop_ponderado(df_train, df_test, campo_group_by, campo_id, campo_label):\n",
        "    df_tmp = pd.DataFrame()\n",
        "    df_tmp_global = pd.DataFrame()\n",
        "    df_tmp['cant_total'] = df_train.groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp_global['cant_total_global'] = df_train.groupby([campo_id],observed=False)[campo_label].size()\n",
        "    df_tmp['cant_likes'] = df_train[df_train[campo_label] == 1].groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp['cant_dislikes'] = df_train[df_train[campo_label] == 0].groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp = df_tmp.fillna(0)\n",
        "    df_tmp = pd.merge(df_tmp, df_tmp_global, how='left', left_index=True, right_index=True)\n",
        "\n",
        "    df_tmp['proporcion'] = ((df_tmp['cant_likes']-df_tmp['cant_dislikes'])*df_tmp['cant_total'])/ (df_tmp['cant_total_global'])\n",
        "\n",
        "    df_tmp['proporcion'] = df_tmp['proporcion'].fillna(value=0)\n",
        "\n",
        "    columnas = list(df_train.groupby(campo_group_by,observed=False).groups.keys())\n",
        "\n",
        "    col_list = ['cant_likes','cant_dislikes','cant_total','cant_total_global']\n",
        "    df_tmp.drop(columns=col_list, inplace=True)\n",
        "    df_tmp = df_tmp.reset_index()\n",
        "    df_pivot = df_tmp.pivot(index=campo_id, columns=campo_group_by, values='proporcion')\n",
        "    df_pivot = df_pivot.reset_index()\n",
        "    df_pivot = df_pivot.fillna(0)\n",
        "\n",
        "\n",
        "    columna_res = campo_group_by + '_' + 'prop_pond'\n",
        "    columnas_campo_group_by = []\n",
        "    for i in range(len(columnas)):\n",
        "    \tcolumnas_campo_group_by.append(campo_group_by + '_' + columnas[i])\n",
        "\n",
        "    df_train = pd.merge(df_train, df_pivot, on=campo_id, how='left')\n",
        "    df_train_dummies = pd.get_dummies(df_train[campo_group_by], prefix=campo_group_by)\n",
        "    df_train = pd.concat([df_train, df_train_dummies], axis=1)\n",
        "    for i in range(len(columnas)):\n",
        "    \tdf_train[columnas[i]] = df_train[[columnas[i],columnas_campo_group_by[i]]].prod(axis=1)\n",
        "    df_train[columna_res] = df_train[columnas].sum(axis=1)\n",
        "    df_train.drop(columns=columnas_campo_group_by, inplace=True)\n",
        "    df_train.drop(columns=columnas, inplace=True)\n",
        "\n",
        "    df_test = pd.merge(df_test, df_pivot, on=campo_id, how='left')\n",
        "    df_test_dummies = pd.get_dummies(df_test[campo_group_by], prefix=campo_group_by)\n",
        "    df_test = pd.concat([df_test, df_test_dummies], axis=1)\n",
        "    for i in range(len(columnas)):\n",
        "    \tdf_test[columnas[i]] = df_test[[columnas[i],columnas_campo_group_by[i]]].prod(axis=1)\n",
        "    df_test[columna_res] = df_test[columnas].sum(axis=1)\n",
        "    df_test.drop(columns=columnas_campo_group_by, inplace=True)\n",
        "    df_test.drop(columns=columnas, inplace=True)\n",
        "\n",
        "    return df_train, df_test"
      ],
      "metadata": {
        "id": "smBwZ9Ko9FrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta es la función que calcula las columnas de frecuencias.\n",
        "\n",
        "Va a servir para crear las siguientes columnas de frecuencias:\n",
        "\n",
        "Cuánto le gustó/no le gustó el género del libro al lector\n",
        "\n",
        "id_lector_generolit_1 id_lector_generolit_0\n",
        "\n",
        "Cuánto le gustó/no le gustó el autor del libro (en minúsculas) al lector\n",
        "\n",
        "id_lector_autor_low_1 id_lector_autor_low_0\n",
        "\n",
        "Cuánto gustó/no gustó en general el género literiario\n",
        "\n",
        "generolit_1 generolit_0\n",
        "\n",
        "Cuánto gustó/no gustó en general el título (en minúsculas)\n",
        "\n",
        "titulo_low_1 titulo_low_0\n",
        "\n",
        "Cuánto gustó/no gustó en general el autor\n",
        "\n",
        "autor_low_1 autor_low_0\n",
        "\n",
        "Cuánto me gusta/no me gusta indicó el usuario\n",
        "\n",
        "id_lector_1 id_lector_0\n",
        "\n",
        "\n",
        "en lista_campos_group_by van los campos que se van a cruzar si son dos: p. ej. autor x lector es ['id_lector', 'autor_low'] (low de lowercase, minúscula)\n",
        "o el campo que se va a usar para agrupar, p. ej. si interesa saber cuantos likes tiene un autor independientemente del usuario, entonces lista_campos_group_by será ['autor_low']"
      ],
      "metadata": {
        "id": "tSVf_wp19M4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_columna_frec(df_train,df_test, lista_campos_group_by, campo_label, valor_label):\n",
        "    colname = '_'.join(lista_campos_group_by)\n",
        "    colname = '_'.join([colname, str(valor_label)])\n",
        "    df_tmp = df_train[df_train[campo_label] == valor_label].groupby(lista_campos_group_by).size().reset_index(name=colname)\n",
        "\n",
        "    df_train = pd.merge(df_train, df_tmp, on=lista_campos_group_by, how='left')\n",
        "    df_train[colname] = df_train[colname].fillna(df_train[colname].mean())\n",
        "    df_train[colname] = df_train[colname].astype(int)\n",
        "    df_test = pd.merge(df_test, df_tmp, on=lista_campos_group_by, how='left')\n",
        "    df_test[colname] = df_test[colname].fillna(df_train[colname].mean())\n",
        "    df_test[colname] = df_test[colname].astype(int)\n",
        "\n",
        "    return df_train, df_test"
      ],
      "metadata": {
        "id": "O50YZd_z9Qh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "frec rel crea frecuencias relativas en lugar de absolutas"
      ],
      "metadata": {
        "id": "qE-LmBGF9S9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_columna_frec_rel(df_train,df_test, lista_campos_group_by, campo_label, valor_label):\n",
        "    colname_pre = '_'.join(lista_campos_group_by)\n",
        "    colname = '_'.join([colname_pre, str(valor_label)])\n",
        "    colname_total = '_'.join([colname,'total'])\n",
        "    colname_frel = '_'.join([colname,'frel'])\n",
        "\n",
        "    df_tmp = df_train[df_train[campo_label] == valor_label].groupby(lista_campos_group_by, observed=False).size().reset_index(name=colname)\n",
        "    df_tmp_total = df_train.groupby(lista_campos_group_by, observed=False).size().reset_index(name=colname_total)\n",
        "\n",
        "\n",
        "    df_tmp_full = pd.merge(df_tmp_total, df_tmp, on=lista_campos_group_by, how='left')\n",
        "    df_tmp_full[colname] = df_tmp_full[colname].fillna(value=0)\n",
        "    df_tmp_full[colname_frel] = df_tmp_full[colname] / df_tmp_full[colname_total]\n",
        "\n",
        "    col_list = lista_campos_group_by + [colname_frel]\n",
        "    df_tmp_full = df_tmp_full[col_list]\n",
        "\n",
        "    df_train = pd.merge(df_train, df_tmp_full, on=lista_campos_group_by, how='left')\n",
        "    df_test = pd.merge(df_test, df_tmp_full, on=lista_campos_group_by, how='left')\n",
        "\n",
        "    df_train[colname_frel] = df_train[colname_frel].fillna(df_train[colname_frel].mean())\n",
        "    df_test[colname_frel] = df_test[colname_frel].fillna(df_test[colname_frel].mean())\n",
        "\n",
        "    return df_train, df_test\n"
      ],
      "metadata": {
        "id": "6FjQRYkM9VAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "frec rel ponderado crea frecuencias \"ponderadas\"\n",
        "proporción = (likes-dislikes)* (likes+dislikes)/(total de opiniones del lector)\n",
        "o lo es lo mismo\n",
        "proporción = (likes-(total-likes)) * total/ (total de opiniones del lector)\n",
        "\n",
        "\n",
        "donde total es likes+dislikes. Por ejemplo, total puede ser la suma de likes y dislikes que introdujo un lector para un determinado género literario."
      ],
      "metadata": {
        "id": "ftxlagxo9YST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_columna_frec_rel_ponderado(df_train,df_test, lista_campos_group_by, campo_label, valor_label):\n",
        "    colname_pre = '_'.join(lista_campos_group_by)\n",
        "    colname = '_'.join([colname_pre, str(valor_label)])\n",
        "    colname_total_global = '_'.join([colname,'total_global'])\n",
        "    colname_total = '_'.join([colname,'total'])\n",
        "    colname_frel = '_'.join([colname,'frel_pond'])\n",
        "\n",
        "    df_tmp = df_train[df_train[campo_label] == valor_label].groupby(lista_campos_group_by, observed=False).size().reset_index(name=colname)\n",
        "    df_tmp_total = df_train.groupby(lista_campos_group_by, observed=False).size().reset_index(name=colname_total)\n",
        "    df_tmp_total_global = df_train.groupby(lista_campos_group_by[0], observed=False).size().reset_index(name=colname_total_global)\n",
        "\n",
        "\n",
        "    df_tmp_full = pd.merge(df_tmp_total, df_tmp, on=lista_campos_group_by, how='left')\n",
        "    df_tmp_full = pd.merge(df_tmp_full, df_tmp_total_global, on=lista_campos_group_by[0], how='left')\n",
        "    df_tmp_full[colname] = df_tmp_full[colname].fillna(value=0)\n",
        "    df_tmp_full[colname_frel] = (df_tmp_full[colname]- (df_tmp_full[colname_total] - df_tmp_full[colname]))*df_tmp_full[colname_total]/df_tmp_full[colname_total_global]\n",
        "\n",
        "\n",
        "    col_list = lista_campos_group_by + [colname_frel]\n",
        "    df_tmp_full = df_tmp_full[col_list]\n",
        "\n",
        "    df_train = pd.merge(df_train, df_tmp_full, on=lista_campos_group_by, how='left')\n",
        "    df_test = pd.merge(df_test, df_tmp_full, on=lista_campos_group_by, how='left')\n",
        "\n",
        "    df_train[colname_frel] = df_train[colname_frel].fillna(df_train[colname_frel].mean())\n",
        "    df_test[colname_frel] = df_test[colname_frel].fillna(df_test[colname_frel].mean())\n",
        "\n",
        "    return df_train, df_test"
      ],
      "metadata": {
        "id": "WFzW2iTw9dD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las siguientes funciones crean una serie de columnas por cada categoría del campo_group_by, por ejemplo, una columna por cada género literario, y cada columna contiene la proporción = likes / (likes+dislikes) , que en el caso del género literario, sería cuánto le gusta (en proporción) cada género a un usuario determinado.\n",
        "\n",
        "Por lo tanto, estas columnas van a contener una serie de valores repetidos, ya que van a ser los mismos para cada usuario."
      ],
      "metadata": {
        "id": "Xay9KEV39aP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_columnas_prop(df_train, df_test, campo_group_by, campo_id, campo_label):\n",
        "    df_tmp = pd.DataFrame()\n",
        "    df_tmp['cant_total'] = df_train.groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp['cant_likes'] = df_train[df_train[campo_label] == 1].groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp['cant_dislikes'] = df_train[df_train[campo_label] == 0].groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp = df_tmp.fillna(0)\n",
        "    df_tmp['proporcion'] = df_tmp['cant_likes']/ (df_tmp['cant_total'])\n",
        "    df_tmp['proporcion'] = df_tmp['proporcion'].fillna(value=0)\n",
        "\n",
        "    col_list = ['cant_likes','cant_dislikes']\n",
        "    df_tmp.drop(columns=col_list, inplace=True)\n",
        "    df_tmp = df_tmp.reset_index()\n",
        "    df_pivot = df_tmp.pivot(index=campo_id, columns=campo_group_by, values='proporcion')\n",
        "    df_pivot = df_pivot.reset_index()\n",
        "    df_pivot = df_pivot.fillna(0)\n",
        "    df_train = pd.merge(df_train, df_pivot, on=campo_id, how='left')\n",
        "\n",
        "    df_test = pd.merge(df_test, df_pivot, on=campo_id, how='left')\n",
        "\n",
        "    return df_train, df_test"
      ],
      "metadata": {
        "id": "2wwAfMNR9ncG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo mismo que la anterior, solo que es útil para usarla con títulos o autores en lugar de género literario, ya que toma los n títulos o autores sobre los cuales hay más opiniones (no serían los más populares, sino los más \"opinados\")."
      ],
      "metadata": {
        "id": "UaZo_kNG9mVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_n_columnas_prop(df_train, df_test, campo_group_by, campo_id, campo_label, n):\n",
        "    df_tmp = pd.DataFrame()\n",
        "    df_tmp['cant_total'] = df_train.groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp['cant_likes'] = df_train[df_train[campo_label] == 1].groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp['cant_dislikes'] = df_train[df_train[campo_label] == 0].groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp = df_tmp.fillna(0)\n",
        "    df_tmp['proporcion'] = df_tmp['cant_likes']/ (df_tmp['cant_total'])\n",
        "    df_tmp['proporcion'] = df_tmp['proporcion'].fillna(value=0)\n",
        "\n",
        "    colname = '_'.join([campo_group_by, 'otros'])\n",
        "\n",
        "    top_n = df_train.groupby(campo_group_by).size().nlargest(n).index\n",
        "\n",
        "    df_tmp = df_tmp.reset_index()\n",
        "    df_tmp['grupo'] = df_tmp[campo_group_by].apply(lambda x: x if x in top_n else colname)\n",
        "\n",
        "    col_list = ['cant_likes','cant_dislikes']\n",
        "    df_tmp.drop(columns=col_list, inplace=True)\n",
        "    df_tmp = df_tmp.reset_index()\n",
        "\n",
        "    df_tmp = df_tmp[~(df_tmp['grupo'] == colname)]\n",
        "\n",
        "\n",
        "    df_pivot = df_tmp.pivot(index=campo_id, columns='grupo', values='proporcion')\n",
        "    df_pivot = df_pivot.reset_index()\n",
        "    df_pivot = df_pivot.fillna(0)\n",
        "\n",
        "    df_train = pd.merge(df_train, df_pivot, on=campo_id, how='left')\n",
        "    df_test = pd.merge(df_test, df_pivot, on=campo_id, how='left')\n",
        "\n",
        "    for columna in list(top_n):\n",
        "        df_train[columna] = df_train[columna].fillna(0)\n",
        "        df_test[columna] = df_test[columna].fillna(0)\n",
        "\n",
        "    return df_train, df_test"
      ],
      "metadata": {
        "id": "A7s-2SL39ucM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En forma similar a las funciones anteriores, solo que en lugar de agregar las columnas, agrega los cant_componentes primeros componentes de un PCA a partir de las columnas como las anteriores. En lugar de seleccionar los n autores más populares, toma a todos, hace un PCA y agrega los primeros componentes a train y test."
      ],
      "metadata": {
        "id": "5AyoF_An9xc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_columnas_prop_pca(df_train, df_test, campo_group_by, campo_id, campo_label, cant_componentes):\n",
        "    df_tmp = pd.DataFrame()\n",
        "    df_tmp['cant_total'] = df_train.groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp['cant_likes'] = df_train[df_train[campo_label] == 1].groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp['cant_dislikes'] = df_train[df_train[campo_label] == 0].groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp = df_tmp.fillna(0)\n",
        "    df_tmp['proporcion'] = df_tmp['cant_likes']/ (df_tmp['cant_total'])\n",
        "    df_tmp['proporcion'] = df_tmp['proporcion'].fillna(value=0)\n",
        "\n",
        "    columnas = list(df_train.groupby(campo_group_by).groups.keys())\n",
        "\n",
        "    col_list = ['cant_likes','cant_dislikes']\n",
        "    df_tmp.drop(columns=col_list, inplace=True)\n",
        "    df_tmp = df_tmp.reset_index()\n",
        "    df_pivot = df_tmp.pivot(index=campo_id, columns=campo_group_by, values='proporcion')\n",
        "    df_pivot = df_pivot.reset_index()\n",
        "    df_pivot = df_pivot.fillna(0)\n",
        "    df_train_backup = df_train.copy()\n",
        "    df_test_backup = df_test.copy()\n",
        "    df_train = pd.merge(df_train, df_pivot, on=campo_id, how='left')\n",
        "    df_test = pd.merge(df_test, df_pivot, on=campo_id, how='left')\n",
        "\n",
        "    df_train_filtrado = df_train[columnas].fillna(0)\n",
        "    df_test_filtrado = df_test[columnas].fillna(0)\n",
        "    pca = PCA(n_components = cant_componentes)\n",
        "    pca.fit(df_train_filtrado)\n",
        "    train_scores = pca.transform(df_train_filtrado)\n",
        "    test_scores = pca.transform(df_test_filtrado)\n",
        "    pca_columns = [f'{campo_group_by}_PC{i+1}' for i in range(train_scores.shape[1])]\n",
        "    df_train_scores = pd.DataFrame(data=train_scores, columns=pca_columns)\n",
        "    df_test_scores = pd.DataFrame(data=test_scores, columns=pca_columns)\n",
        "    df_train_res = pd.concat([df_train_backup, df_train_scores], axis=1)\n",
        "    df_test_res = pd.concat([df_test_backup, df_test_scores], axis=1)\n",
        "    return df_train_res, df_test_res"
      ],
      "metadata": {
        "id": "VfdPgCGH91cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar a la función anterior, solo que toma los n valores más opinados de la variable categórica para hacer el PCA. Es decir, en lugar de hacer el PCA con todos los autores, lo hace con los 500 (si n=500) sobre los cuales más se ha opinado."
      ],
      "metadata": {
        "id": "mFRXF0uC-BQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_columnas_prop_pca_acotado(df_train, df_test, campo_group_by, campo_id, campo_label, cant_componentes, n):\n",
        "    df_tmp = pd.DataFrame()\n",
        "    df_tmp['cant_total'] = df_train.groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp['cant_likes'] = df_train[df_train[campo_label] == 1].groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp['cant_dislikes'] = df_train[df_train[campo_label] == 0].groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp = df_tmp.fillna(0)\n",
        "    df_tmp['proporcion'] = df_tmp['cant_likes']/ (df_tmp['cant_total'])\n",
        "    df_tmp['proporcion'] = df_tmp['proporcion'].fillna(value=0)\n",
        "\n",
        "    colname = '_'.join([campo_group_by, 'otros'])\n",
        "\n",
        "    top_n = df_train.groupby(campo_group_by).size().nlargest(n).index\n",
        "    columnas = list(top_n)\n",
        "    df_tmp = df_tmp.reset_index()\n",
        "    df_tmp['grupo'] = df_tmp[campo_group_by].apply(lambda x: x if x in top_n else colname)\n",
        "\n",
        "    col_list = ['cant_likes','cant_dislikes']\n",
        "    df_tmp.drop(columns=col_list, inplace=True)\n",
        "    df_tmp = df_tmp.reset_index()\n",
        "\n",
        "    df_tmp = df_tmp[~(df_tmp['grupo'] == colname)]\n",
        "    df_tmp = df_tmp.reset_index()\n",
        "    df_pivot = df_tmp.pivot(index=campo_id, columns=campo_group_by, values='proporcion')\n",
        "    df_pivot = df_pivot.reset_index()\n",
        "    df_pivot = df_pivot.fillna(0)\n",
        "    df_train_backup = df_train.copy()\n",
        "    df_test_backup = df_test.copy()\n",
        "    df_train = pd.merge(df_train, df_pivot, on=campo_id, how='left')\n",
        "    df_test = pd.merge(df_test, df_pivot, on=campo_id, how='left')\n",
        "    df_train_filtrado = df_train[columnas].fillna(0)\n",
        "    df_test_filtrado = df_test[columnas].fillna(0)\n",
        "    pca = PCA(n_components = cant_componentes)\n",
        "    pca.fit(df_train_filtrado)\n",
        "    train_scores = pca.transform(df_train_filtrado)\n",
        "    test_scores = pca.transform(df_test_filtrado)\n",
        "    pca_columns = [f'{campo_group_by}_PC{i+1}' for i in range(train_scores.shape[1])]\n",
        "    df_train_scores = pd.DataFrame(data=train_scores, columns=pca_columns)\n",
        "    df_test_scores = pd.DataFrame(data=test_scores, columns=pca_columns)\n",
        "    df_train_res = pd.concat([df_train_backup, df_train_scores], axis=1)\n",
        "    df_test_res = pd.concat([df_test_backup, df_test_scores], axis=1)\n",
        "    return df_train_res, df_test_res"
      ],
      "metadata": {
        "id": "juDPAYrv-YTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las siguientes funciones son iguales a las anteriores solo que calculan una versión \"ponderada\" de la proporción de likes.\n",
        "\n",
        "proporción = (likes-dislikes)*(likes+dislikes)/total de opiniones del usuario\n",
        "la idea de esta ponderación es que la medida elegida arroje un valor positivo si hay más likes que dislikes y negativo si hay más dislikes que likes.\n",
        "\n",
        "Luego lo multiplica por la suma de likes y dislikes y divide por el total de opiniones del usuario para \"ponderarlo\" por cuánto ha opinado el usuario sobre ese autor, por ejemplo, respecto del total de sus opiniones."
      ],
      "metadata": {
        "id": "WgRREFGc-bWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_columnas_prop_ponderado(df_train, df_test, campo_group_by, campo_id, campo_label):\n",
        "    df_tmp = pd.DataFrame()\n",
        "    df_tmp_global = pd.DataFrame()\n",
        "    df_tmp['cant_total'] = df_train.groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp_global['cant_total_global'] = df_train.groupby([campo_id],observed=False)[campo_label].size()\n",
        "    df_tmp['cant_likes'] = df_train[df_train[campo_label] == 1].groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp['cant_dislikes'] = df_train[df_train[campo_label] == 0].groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp = df_tmp.fillna(0)\n",
        "    df_tmp = pd.merge(df_tmp, df_tmp_global, how='left', left_index=True, right_index=True)\n",
        "\n",
        "    df_tmp['proporcion'] = ((df_tmp['cant_likes']-df_tmp['cant_dislikes'])*df_tmp['cant_total'])/ (df_tmp['cant_total_global'])\n",
        "\n",
        "    df_tmp['proporcion'] = df_tmp['proporcion'].fillna(value=0)\n",
        "\n",
        "    col_list = ['cant_likes','cant_dislikes','cant_total','cant_total_global']\n",
        "    df_tmp.drop(columns=col_list, inplace=True)\n",
        "    df_tmp = df_tmp.reset_index()\n",
        "    df_pivot = df_tmp.pivot(index=campo_id, columns=campo_group_by, values='proporcion')\n",
        "    df_pivot = df_pivot.reset_index()\n",
        "    df_pivot = df_pivot.fillna(0)\n",
        "    df_train = pd.merge(df_train, df_pivot, on=campo_id, how='left')\n",
        "\n",
        "    df_test = pd.merge(df_test, df_pivot, on=campo_id, how='left')\n",
        "\n",
        "    return df_train, df_test\n",
        "\n",
        "def crear_n_columnas_prop_ponderado(df_train, df_test, campo_group_by, campo_id, campo_label, n):\n",
        "    df_tmp = pd.DataFrame()\n",
        "    df_tmp_global = pd.DataFrame()\n",
        "    df_tmp['cant_total'] = df_train.groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp_global['cant_total_global'] = df_train.groupby([campo_id],observed=False)[campo_label].size()\n",
        "    df_tmp['cant_likes'] = df_train[df_train[campo_label] == 1].groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp['cant_dislikes'] = df_train[df_train[campo_label] == 0].groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp = df_tmp.fillna(0)\n",
        "    df_tmp = pd.merge(df_tmp, df_tmp_global, how='left', left_index=True, right_index=True)\n",
        "\n",
        "    df_tmp['proporcion'] = ((df_tmp['cant_likes']-df_tmp['cant_dislikes'])*df_tmp['cant_total'])/ (df_tmp['cant_total_global'])\n",
        "\n",
        "    df_tmp['proporcion'] = df_tmp['proporcion'].fillna(value=0)\n",
        "\n",
        "    colname = '_'.join([campo_group_by, 'otros'])\n",
        "\n",
        "    top_n = df_train.groupby(campo_group_by).size().nlargest(n).index\n",
        "\n",
        "    df_tmp = df_tmp.reset_index()\n",
        "    df_tmp['grupo'] = df_tmp[campo_group_by].apply(lambda x: x if x in top_n else colname)\n",
        "\n",
        "    col_list = ['cant_likes','cant_dislikes']\n",
        "    df_tmp.drop(columns=col_list, inplace=True)\n",
        "    df_tmp = df_tmp.reset_index()\n",
        "\n",
        "    df_tmp = df_tmp[~(df_tmp['grupo'] == colname)]\n",
        "\n",
        "\n",
        "    df_pivot = df_tmp.pivot(index=campo_id, columns='grupo', values='proporcion')\n",
        "    df_pivot = df_pivot.reset_index()\n",
        "    df_pivot = df_pivot.fillna(0)\n",
        "\n",
        "    df_train = pd.merge(df_train, df_pivot, on=campo_id, how='left')\n",
        "    df_test = pd.merge(df_test, df_pivot, on=campo_id, how='left')\n",
        "\n",
        "    for columna in list(top_n):\n",
        "        df_train[columna] = df_train[columna].fillna(0)\n",
        "        df_test[columna] = df_test[columna].fillna(0)\n",
        "\n",
        "    return df_train, df_test\n",
        "\n",
        "def crear_columnas_prop_pca_ponderado(df_train, df_test, campo_group_by, campo_id, campo_label, cant_componentes):\n",
        "    df_tmp = pd.DataFrame()\n",
        "    df_tmp_global = pd.DataFrame()\n",
        "    df_tmp['cant_total'] = df_train.groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp_global['cant_total_global'] = df_train.groupby([campo_id],observed=False)[campo_label].size()\n",
        "    df_tmp['cant_likes'] = df_train[df_train[campo_label] == 1].groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp['cant_dislikes'] = df_train[df_train[campo_label] == 0].groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp = df_tmp.fillna(0)\n",
        "    df_tmp = pd.merge(df_tmp, df_tmp_global, how='left', left_index=True, right_index=True)\n",
        "\n",
        "    df_tmp['proporcion'] = ((df_tmp['cant_likes']-df_tmp['cant_dislikes'])*df_tmp['cant_total'])/ (df_tmp['cant_total_global'])\n",
        "\n",
        "    df_tmp['proporcion'] = df_tmp['proporcion'].fillna(value=0)\n",
        "\n",
        "    columnas = list(df_train.groupby(campo_group_by,observed=False).groups.keys())\n",
        "\n",
        "    col_list = ['cant_likes','cant_dislikes']\n",
        "    df_tmp.drop(columns=col_list, inplace=True)\n",
        "    df_tmp = df_tmp.reset_index()\n",
        "    df_pivot = df_tmp.pivot(index=campo_id, columns=campo_group_by, values='proporcion')\n",
        "    df_pivot = df_pivot.reset_index()\n",
        "    df_pivot = df_pivot.fillna(0)\n",
        "    df_train_backup = df_train.copy()\n",
        "    df_test_backup = df_test.copy()\n",
        "    df_train = pd.merge(df_train, df_pivot, on=campo_id, how='left')\n",
        "    df_test = pd.merge(df_test, df_pivot, on=campo_id, how='left')\n",
        "\n",
        "    df_train_filtrado = df_train[columnas].fillna(0)\n",
        "    df_test_filtrado = df_test[columnas].fillna(0)\n",
        "    pca = PCA(n_components = cant_componentes)\n",
        "    pca.fit(df_train_filtrado)\n",
        "    train_scores = pca.transform(df_train_filtrado)\n",
        "    test_scores = pca.transform(df_test_filtrado)\n",
        "    pca_columns = [f'{campo_group_by}_PC{i+1}' for i in range(train_scores.shape[1])]\n",
        "    df_train_scores = pd.DataFrame(data=train_scores, columns=pca_columns)\n",
        "    df_test_scores = pd.DataFrame(data=test_scores, columns=pca_columns)\n",
        "    df_train_res = pd.concat([df_train_backup, df_train_scores], axis=1)\n",
        "    df_test_res = pd.concat([df_test_backup, df_test_scores], axis=1)\n",
        "    return df_train_res, df_test_res\n",
        "\n",
        "def crear_columnas_prop_pca_acotado_ponderado(df_train, df_test, campo_group_by, campo_id, campo_label, cant_componentes, n):\n",
        "    df_tmp = pd.DataFrame()\n",
        "    df_tmp_global = pd.DataFrame()\n",
        "    df_tmp['cant_total'] = df_train.groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp_global['cant_total_global'] = df_train.groupby([campo_id],observed=False)[campo_label].size()\n",
        "    df_tmp['cant_likes'] = df_train[df_train[campo_label] == 1].groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp['cant_dislikes'] = df_train[df_train[campo_label] == 0].groupby([campo_id, campo_group_by],observed=False)[campo_label].size()\n",
        "    df_tmp = df_tmp.fillna(0)\n",
        "    df_tmp = pd.merge(df_tmp, df_tmp_global, how='left', left_index=True, right_index=True)\n",
        "\n",
        "    df_tmp['proporcion'] = ((df_tmp['cant_likes']-df_tmp['cant_dislikes'])*df_tmp['cant_total'])/ (df_tmp['cant_total_global'])\n",
        "\n",
        "    df_tmp['proporcion'] = df_tmp['proporcion'].fillna(value=0)\n",
        "\n",
        "    colname = '_'.join([campo_group_by, 'otros'])\n",
        "\n",
        "    top_n = df_train.groupby(campo_group_by).size().nlargest(n).index\n",
        "    columnas = list(top_n)\n",
        "    df_tmp = df_tmp.reset_index()\n",
        "    df_tmp['grupo'] = df_tmp[campo_group_by].apply(lambda x: x if x in top_n else colname)\n",
        "\n",
        "    col_list = ['cant_likes','cant_dislikes']\n",
        "    df_tmp.drop(columns=col_list, inplace=True)\n",
        "    df_tmp = df_tmp.reset_index()\n",
        "\n",
        "    df_tmp = df_tmp[~(df_tmp['grupo'] == colname)]\n",
        "    df_tmp = df_tmp.reset_index()\n",
        "    df_pivot = df_tmp.pivot(index=campo_id, columns=campo_group_by, values='proporcion')\n",
        "    df_pivot = df_pivot.reset_index()\n",
        "    df_pivot = df_pivot.fillna(0)\n",
        "    df_train_backup = df_train.copy()\n",
        "    df_test_backup = df_test.copy()\n",
        "    df_train = pd.merge(df_train, df_pivot, on=campo_id, how='left')\n",
        "    df_test = pd.merge(df_test, df_pivot, on=campo_id, how='left')\n",
        "    df_train_filtrado = df_train[columnas].fillna(0)\n",
        "    df_test_filtrado = df_test[columnas].fillna(0)\n",
        "    pca = PCA(n_components = cant_componentes)\n",
        "    pca.fit(df_train_filtrado)\n",
        "    train_scores = pca.transform(df_train_filtrado)\n",
        "    test_scores = pca.transform(df_test_filtrado)\n",
        "    pca_columns = [f'{campo_group_by}_PC{i+1}' for i in range(train_scores.shape[1])]\n",
        "    df_train_scores = pd.DataFrame(data=train_scores, columns=pca_columns)\n",
        "    df_test_scores = pd.DataFrame(data=test_scores, columns=pca_columns)\n",
        "    df_train_res = pd.concat([df_train_backup, df_train_scores], axis=1)\n",
        "    df_test_res = pd.concat([df_test_backup, df_test_scores], axis=1)\n",
        "    return df_train_res, df_test_res\n"
      ],
      "metadata": {
        "id": "YK5R5ZX3-djb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tanto la base de datos de lectores como de libros han sido modificadas.\n",
        "La de lectores tiene imputado el campo de género (sexo) con los valores más probables o razonables en función del nombre del usuario\n",
        "\n",
        "la de libros es el resultado de un scraping de quelibroleo.com que permitió agregar el valor promedio de las valoraciones de los usuarios, y la cantidad de valoraciones para el rango de 0 a 10."
      ],
      "metadata": {
        "id": "a7_GDxkn-mGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIR = \"/content/drive/MyDrive/Clases/FCEN DM/datos/qll\"\n",
        "df_train = pd.read_csv(f\"{DIR}/entrenamiento.csv\")\n",
        "df_test = pd.read_csv(f\"{DIR}/prueba.csv\")\n",
        "df_lectores  = pd.read_csv(f\"{DIR}/lectores.csv\")\n",
        "df_libros = pd.read_csv(f\"{DIR}/libros-nuevo.csv\")\n",
        "\n",
        "\n",
        "\n",
        "df_train_orig = df_train.copy()\n",
        "df_test_orig = df_test.copy()"
      ],
      "metadata": {
        "id": "rr1cpw8G-sl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seleccionamos de train solo los lectores que aparecen en test"
      ],
      "metadata": {
        "id": "b9Wpjn1A-vdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_id_lector = set(df_train['id_lector'].unique())\n",
        "df_test_id_lector = set(df_test['id_lector'].unique())\n",
        "id_lector_unicos_train = df_train_id_lector - df_test_id_lector\n",
        "id_lector_comunes = df_train_id_lector.intersection(df_test_id_lector)\n",
        "df_train = df_train[df_train['id_lector'].isin(list(id_lector_comunes))]\n",
        "#df_train = df_train[~df_train['id_lector'].isin(list(id_lector_unicos_train))]\n",
        "\n",
        "#el campo rating value está codificado como string, lo pasamos a float, e imputamos los faltantes con la media\n",
        "df_libros['rating_value'] = df_libros['rating_value'].astype(float)\n",
        "df_libros['rating_value'] = df_libros['rating_value'].apply(pd.to_numeric)\n",
        "df_libros['rating_value'] = df_libros['rating_value'].fillna(df_libros['rating_value'].mean())\n",
        "\n",
        "#estos campos con conteos están codificados como strings, los pasamos a entero\n",
        "columnas_a_entero = ['best_rating', 'worst_rating', 'rating_count',\n",
        "                    'cant_votos_1', 'cant_votos_2', 'cant_votos_3', 'cant_votos_4',\n",
        "                    'cant_votos_5', 'cant_votos_6', 'cant_votos_7', 'cant_votos_8',\n",
        "                    'cant_votos_9', 'cant_votos_10']\n",
        "df_libros[columnas_a_entero] = df_libros[columnas_a_entero].apply(lambda x: pd.to_numeric(x, downcast='integer'))\n",
        "\n",
        "df_libros.rename(columns={'genero': 'generolit'}, inplace=True)\n",
        "df_libros.drop_duplicates(subset=['id_libro'], keep='first', inplace=True)\n",
        "df_lectores['nacimiento'] = df_lectores['nacimiento'].replace(1910, np.nan)\n",
        "df_lectores['nacimiento'] = df_lectores['nacimiento'].fillna(df_lectores['nacimiento'].median())\n",
        "df_libros['generolit'] = df_libros['generolit'].fillna('generolit_desconocido')\n",
        "df_libros['autor'] = df_libros['autor'].fillna('autor_desconocido')\n",
        "\n",
        "df_libros['anio_edicion'] = pd.to_numeric(df_libros['anio_edicion'], errors='coerce')\n",
        "df_libros['anio_edicion'] = df_libros['anio_edicion'].fillna(df_libros['anio_edicion'].median())\n",
        "\n",
        "nbins=10\n",
        "df_libros['anio_edicion_rango'] = pd.qcut(df_libros['anio_edicion'], q=nbins, labels=[str(x) for x in range(nbins)], precision=0, duplicates='drop')"
      ],
      "metadata": {
        "id": "NjcuZ6BE-yBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un intento de corregir algunos n/as de la base de libros, que quedó abandonado..."
      ],
      "metadata": {
        "id": "1cJS5H5x-18m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_libros.loc[df_libros['id_libro'] == 'el-gran-gatsby', 'titulo'] = df_libros.loc[df_libros['id_libro'] == 'el-gran-gatsby-2', 'titulo'].values[0]\n",
        "df_libros.loc[df_libros['id_libro'] == 'el-gran-gatsby', 'autor'] = df_libros.loc[df_libros['id_libro'] == 'el-gran-gatsby-2', 'autor'].values[0]\n",
        "df_libros.loc[df_libros['id_libro'] == 'el-gran-gatsby', 'generolit'] = df_libros.loc[df_libros['id_libro'] == 'el-gran-gatsby-2', 'generolit'].values[0]"
      ],
      "metadata": {
        "id": "NlTz3Sdl-4PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos el join de los dataframes"
      ],
      "metadata": {
        "id": "lyESjeG1-6NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_extendido = pd.merge(df_train, df_lectores, on='id_lector', how='left').merge(df_libros, on='id_libro', how='left')\n",
        "df_test_extendido = pd.merge(df_test, df_lectores, on='id_lector', how='left').merge(df_libros, on='id_libro', how='left')"
      ],
      "metadata": {
        "id": "K7nAOosi-8nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limpiamos y amalgamamos los géneros literiarios en train"
      ],
      "metadata": {
        "id": "KrCDL359--4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='varios', value='Varios')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Histórica Y Aventuras', value='Histórica y aventuras')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='histórica y aventuras', value='Histórica y aventuras')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Biografiás, Memorias', value='Biografías, Memorias')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Clasicos de la literatura', value='Clásicos de la literatura')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Novela Negra, Intriga, Terror', value='Novela negra')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Novela negra, intriga, terror', value='Novela negra')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='No Ficción', value='No ficción')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Ensayo', value='Estudios y ensayos')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Economía financiera', value='Economía')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Biografías', value='Biografías, Memorias')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Poesía, teatro', value='Poesía, Teatro')\n",
        "\n",
        "#para igualar géneros en train y test\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Actores', value='Varios')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Administración y dirección empresarial', value='Empresa')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Autoayuda', value='Autoayuda Y Espiritualidad')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Ciencias Políticas Y Sociales', value='Ciencias Humanas')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Cómics', value='Cómics, Novela Gráfica')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Derecho', value='Ciencias Humanas')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Didáctica y metodología', value='Ciencias Humanas')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Dietética y nutrición', value='Varios')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Fotografía', value='Varios')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Guías De Viaje', value='Varios')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Historia moderna de España', value='Historia')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Histórica', value='Narrativa histórica')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Matemáticas divulgativas', value='Ciencias')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Medicina', value='Ciencias')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Naturaleza y ciencia', value='Ciencias')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Poesía', value='Poesía, Teatro')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Policiaca. Novela negra en bolsillo', value='Novela negra')\n",
        "df_train_extendido.generolit = df_train_extendido.generolit.replace(to_replace='Televisión', value='Varios')\n",
        "\n",
        "df_train_extendido['generolit'] = df_train_extendido['generolit'].astype('category')\n",
        "df_train_extendido['genero'] = df_train_extendido['genero'].astype('category')"
      ],
      "metadata": {
        "id": "Vea7qkYL_D_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limpiamos y amalgamamos los géneros literiarios en test"
      ],
      "metadata": {
        "id": "3EnyKUJ8_HGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_extendido.generolit = df_test_extendido.generolit.replace(to_replace='Romántica, Erótica', value='Romántica, erótica')\n",
        "df_test_extendido.generolit = df_test_extendido.generolit.replace(to_replace='varios', value='Varios')\n",
        "df_test_extendido.generolit = df_test_extendido.generolit.replace(to_replace='Histórica Y Aventuras', value='Histórica y aventuras')\n",
        "df_test_extendido.generolit = df_test_extendido.generolit.replace(to_replace='histórica y aventuras', value='Histórica y aventuras')\n",
        "df_test_extendido.generolit = df_test_extendido.generolit.replace(to_replace='Biografiás, Memorias', value='Biografías, Memorias')\n",
        "df_test_extendido.generolit = df_test_extendido.generolit.replace(to_replace='Clasicos de la literatura', value='Clásicos de la literatura')\n",
        "df_test_extendido.generolit = df_test_extendido.generolit.replace(to_replace='Novela Negra, Intriga, Terror', value='Novela negra')\n",
        "df_test_extendido.generolit = df_test_extendido.generolit.replace(to_replace='Novela negra, intriga, terror', value='Novela negra')\n",
        "df_test_extendido.generolit = df_test_extendido.generolit.replace(to_replace='No Ficción', value='No ficción')\n",
        "df_test_extendido.generolit = df_test_extendido.generolit.replace(to_replace='Ensayo', value='Estudios y ensayos')\n",
        "df_test_extendido.generolit = df_test_extendido.generolit.replace(to_replace='Economía financiera', value='Economía')\n",
        "df_test_extendido.generolit = df_test_extendido.generolit.replace(to_replace='Biografías', value='Biografías, Memorias')\n",
        "df_test_extendido.generolit = df_test_extendido.generolit.replace(to_replace='Poesía, teatro', value='Poesía, Teatro')\n",
        "\n",
        "#para igualar géneros en train y test\n",
        "df_test_extendido.generolit = df_test_extendido.generolit.replace(to_replace='Informática', value='Ciencias')\n",
        "df_test_extendido.generolit = df_test_extendido.generolit.replace(to_replace='Medicina divulgativa', value='Varios')\n",
        "df_test_extendido.generolit = df_test_extendido.generolit.replace(to_replace='Política nacional', value='Varios')\n"
      ],
      "metadata": {
        "id": "IsK9zsR5_JPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos dummies para género literario. Generamos una dummie para el género (sexo) del lector. Pasamos los nombres de autor y títulos de los libros a minúsculas en nuevas columnas (_low de lowercase)."
      ],
      "metadata": {
        "id": "xKNwPcGZ_L8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_extendido['generolit'] = df_test_extendido['generolit'].astype('category')\n",
        "df_test_extendido['genero'] = df_test_extendido['genero'].astype('category')\n",
        "\n",
        "df_train_extendido_dummies_genero = pd.get_dummies(df_train_extendido['genero'], prefix='genero', drop_first = True)\n",
        "df_test_extendido_dummies_genero = pd.get_dummies(df_test_extendido['genero'], prefix='genero', drop_first = True)\n",
        "df_train_extendido_dummies_generolit = pd.get_dummies(df_train_extendido['generolit'], prefix='generolit')\n",
        "df_test_extendido_dummies_generolit = pd.get_dummies(df_test_extendido['generolit'], prefix='generolit')\n",
        "\n",
        "df_train_vars, df_test_vars = df_train_extendido, df_test_extendido\n",
        "\n",
        "df_train_extendido['autor_low'] = df_train_extendido['autor'].str.lower()\n",
        "df_test_extendido['autor_low'] = df_test_extendido['autor'].str.lower()\n",
        "df_train_extendido['titulo_low'] = df_train_extendido['titulo'].str.lower()\n",
        "df_test_extendido['titulo_low'] = df_test_extendido['titulo'].str.lower()\n",
        "df_train_extendido['editorial_low'] = df_train_extendido['editorial'].str.lower()\n",
        "df_test_extendido['editorial_low'] = df_test_extendido['editorial'].str.lower()"
      ],
      "metadata": {
        "id": "jyV9Lulv_OaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos las columnas con frecuencias tal como describimos más arriba, usando la función crear_columna_frec.\n",
        "En esta versión usamos las frecuencias absolutas: cantidad de likes y cantidad de dislikes"
      ],
      "metadata": {
        "id": "ST8kKXCH_RR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_campos = [['generolit'],['titulo_low'],['autor_low']]\n",
        "\n",
        "for campos in lista_campos:\n",
        "    df_train_vars, df_test_vars = crear_columna_frec(df_train_vars, df_test_vars,campos, 'label', 1 )\n",
        "    df_train_vars, df_test_vars = crear_columna_frec(df_train_vars, df_test_vars,campos, 'label', 0 )\n",
        "\n",
        "lista_campos = [['id_lector','generolit'],['id_lector','autor_low'],['id_lector','editorial_low'],['id_lector','anio_edicion_rango']]\n",
        "\n",
        "for campos in lista_campos:\n",
        "    df_train_vars, df_test_vars = crear_columna_frec(df_train_vars, df_test_vars,campos, 'label', 1 )\n",
        "    df_train_vars, df_test_vars = crear_columna_frec(df_train_vars, df_test_vars,campos, 'label', 0 )"
      ],
      "metadata": {
        "id": "DthV7fNM_TWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculamos las columnas con proporciones de likes de género literario. Incluímos el primer eje de PCA de título, autor, editorial, año de edición."
      ],
      "metadata": {
        "id": "PDRzrspk_WOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_generolit, df_test_generolit = crear_columnas_prop_ponderado(df_train_vars, df_test_vars, 'generolit', 'id_lector', 'label')\n",
        "df_train_titulo, df_test_titulo = crear_columnas_prop_pca_acotado_ponderado(df_train_generolit, df_test_generolit, 'titulo_low', 'id_lector', 'label',2,1500)\n",
        "df_train_autor, df_test_autor = crear_columnas_prop_pca_acotado_ponderado(df_train_titulo, df_test_titulo, 'autor_low', 'id_lector', 'label',1,500)\n",
        "df_train_editorial, df_test_editorial = crear_columnas_prop_pca_ponderado(df_train_autor, df_test_autor, 'editorial_low', 'id_lector', 'label',1)\n",
        "df_train_anio_edicion_rango, df_test_anio_edicion_rango = crear_columnas_prop_pca_ponderado(df_train_editorial, df_test_editorial, 'anio_edicion_rango', 'id_lector', 'label',1)"
      ],
      "metadata": {
        "id": "-A4vnWqX_Vmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incluímos las dummies en el modelo"
      ],
      "metadata": {
        "id": "c4Jc1XYp_cV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_extendido_con_dummies = pd.concat([df_train_anio_edicion_rango, df_train_extendido_dummies_genero, df_train_extendido_dummies_generolit], axis=1)\n",
        "df_test_extendido_con_dummies = pd.concat([df_test_anio_edicion_rango, df_test_extendido_dummies_genero, df_test_extendido_dummies_generolit], axis=1)\n",
        "\n",
        "\n",
        "df_train = df_train_extendido_con_dummies\n",
        "df_test = df_test_extendido_con_dummies\n",
        "\n",
        "df_train = df_train.select_dtypes(include=['float64', 'int64', 'int32', 'int16', 'int8', 'bool'])"
      ],
      "metadata": {
        "id": "JXkxOqR8_kLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descartamos las columnas id_lector y año de edición de train. Descartamos las columnas de mejor y peor puntaje y cantidad total de opiniones"
      ],
      "metadata": {
        "id": "Tf2RT1av_mei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_list = ['id_lector','anio_edicion','best_rating', 'worst_rating', 'rating_count']\n",
        "\n",
        "df_train.drop(columns=col_list, inplace=True)\n",
        "\n",
        "\n",
        "## Datos a predecir\n",
        "X = df_train[df_train.columns.drop('label')]\n",
        "y = df_train['label']\n"
      ],
      "metadata": {
        "id": "za4-EruX_oSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para ajustar los hiperparámetros, ahora comentado, no se ejecuta.\n",
        "\n",
        "El score F1 daba muy alto, por encima de 0,9."
      ],
      "metadata": {
        "id": "ex2FGwey_rSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resultados = []\n",
        "\n",
        "# for n_estimators in [50, 100, 500, 1000]:\n",
        "#     for max_depth in [5, 10, 15, 30]:\n",
        "#         print(f\"{n_estimators=} -- {max_depth=}\")\n",
        "\n",
        "#         # Creamos el modelo\n",
        "#         clf = sk.ensemble.RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, n_jobs=-1, random_state=42)\n",
        "\n",
        "#         scores_train = []\n",
        "#         scores_test = []\n",
        "\n",
        "#         # Validación cruzada, 10 folds, shuffle antes, semilla aleatoria\n",
        "#         kf = sk.model_selection.KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "#         for fold, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
        "#             # Partimos el fold en entrenamiento y prueba...\n",
        "#             X_train, X_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "#             # Entrenamos el modelo en entramiento\n",
        "#             clf.fit(X_train, y_train)\n",
        "\n",
        "#             # Predecimos en train\n",
        "#             y_pred = clf.predict(X_train)\n",
        "\n",
        "#             # Medimos la performance de la predicción en entramiento\n",
        "#             score_train = sk.metrics.f1_score(y_train, y_pred)\n",
        "#             scores_train.append(score_train)\n",
        "\n",
        "#             # Predecimos en test\n",
        "#             y_pred = clf.predict(X_test)\n",
        "\n",
        "#             # Medimos la performance de la predicción en prueba\n",
        "#             score_test = sk.metrics.f1_score(y_test, y_pred)\n",
        "#             scores_test.append(score_test)\n",
        "\n",
        "#             print(\"\\t\", f\"{fold=}, {score_train=} {score_test=}\")\n",
        "#             media_scores_entrenamiento = pd.Series(scores_train).mean()\n",
        "#             std_scores_entrenamiento = pd.Series(scores_train).std()\n",
        "#             media_scores_prueba = pd.Series(scores_test).mean()\n",
        "#             std_scores_prueba=pd.Series(scores_test).std()\n",
        "\n",
        "#         print(f\"Media de scores en entrenamiento={media_scores_entrenamiento}, std={std_scores_entrenamiento}\")\n",
        "#         print(f\"Media de scores en prueba={media_scores_prueba}, std={std_scores_prueba}\")\n",
        "#         print()\n",
        "#         resultados.append([n_estimators, max_depth, media_scores_entrenamiento, std_scores_entrenamiento, media_scores_prueba, std_scores_prueba])\n"
      ],
      "metadata": {
        "id": "fqvZz1wQ_twr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos un cambio en los parámetros del RandomForestClassifier tal como sugirió José Chelquer, de setear max_features = 2, a fin de reducir el overfitting.\n",
        "Está comentado el código que empleamos cuando probamos los remuestreos para datos desbalanceados"
      ],
      "metadata": {
        "id": "OZ3SIOEQ_w6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = df_test[df_train.columns.drop('label')]\n",
        "\n",
        "# Entrenamos el modelo usando todos los datos de entrenamiento\n",
        "# TODO: Poner los valores de hiperparámetros que mejor dieron en el paso anterior\n",
        "n_estimators = 1000\n",
        "max_depth = 15\n",
        "clf = sk.ensemble.RandomForestClassifier(max_features= 2,min_samples_split = 20, min_samples_leaf = 10, n_estimators=n_estimators, max_depth=max_depth, n_jobs=-1, random_state=42)\n",
        "\n",
        "#enn = im.under_sampling.EditedNearestNeighbours(sampling_strategy='majority')\n",
        "\n",
        "#X, y = enn.fit_resample(X, y)\n",
        "\n",
        "#ros = im.over_sampling.RandomOverSampler(random_state=42, shrinkage=0.1)\n",
        "#X, y = ros.fit_resample(X, y)\n",
        "\n",
        "#clf = sk.ensemble.RandomForestClassifier(class_weight=\"balanced\", n_estimators=n_estimators, max_depth=max_depth, n_jobs=-1, random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Predecimos\n",
        "\n",
        "df_test['label'] = clf.predict(X_test)\n",
        "#df_test['label'] = clf.predict_proba(X_test)[:,1]\n",
        "\n",
        "#proba1 = clf.predict_proba(X_test)[:,1]\n",
        "#umbral = 0.4\n",
        "#df_test['label'] = np.where(proba1 > umbral, 1, 0)\n",
        "\n",
        "\n",
        "df_test['id'] = df_test['id_lector'].astype(str) + \"--\" + df_test['id_libro'].astype(str)\n",
        "df_test.set_index('id', inplace=True)\n",
        "\n",
        "# Creamos el dataframe para entregar\n",
        "df_sol = df_test[[\"label\"]]\n",
        "\n",
        "# Tests de validación de la predicción antes de subirla\n",
        "# Estos tests TIENEN que pasar sin error\n",
        "\n",
        "\n",
        "assert df_sol.shape[0] == 10332, f\"La cantidad de filas no es correcta. Es {df_sol.shape[0]} y debe ser 10332.\"\n",
        "assert df_sol.shape[1] == 1, f\"La cantidad de columnas no es correcta. Es {df_sol.shape[1]} y debe ser 1.\"\n",
        "assert 'label' in df_sol.columns, \"Falta la columna 'label'.\"\n",
        "assert df_sol.index.name == 'id', \"El índice debe llamarse 'id'.\"\n"
      ],
      "metadata": {
        "id": "R9RmFGlk_zbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardamos la respuesta, vemos la importancia de las columnas en el modelo e imprimimos la cantidad de ceros en a_predecir"
      ],
      "metadata": {
        "id": "IGKMADLc_3bW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "version = \"v008i-final\"\n",
        "df_test['label'].to_csv(f\"{DIR}/solucion-{version}.csv\", index=True)\n",
        "\n",
        "imp = pd.DataFrame({\n",
        "    \"feature\": clf.feature_names_in_,\n",
        "    \"importance\": clf.feature_importances_\n",
        "})\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "print(imp.sort_values(by=\"importance\", ascending=False))\n",
        "\n",
        "print((df_test['label'] == 0).sum())\n",
        "#accuracy_precision_recall(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Tx_rs8E_5mu",
        "outputId": "9443ee4d-c4db-4f7c-a97e-a0404c9a4d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  feature    importance\n",
            "21                  id_lector_autor_low_0  2.024240e-01\n",
            "20                  id_lector_autor_low_1  1.474982e-01\n",
            "1                            rating_value  9.182590e-02\n",
            "14                           titulo_low_1  6.546886e-02\n",
            "15                           titulo_low_0  6.358406e-02\n",
            "16                            autor_low_1  3.363039e-02\n",
            "17                            autor_low_0  3.006716e-02\n",
            "9                            cant_votos_8  2.695260e-02\n",
            "8                            cant_votos_7  2.373243e-02\n",
            "10                           cant_votos_9  2.333871e-02\n",
            "7                            cant_votos_6  1.855361e-02\n",
            "23              id_lector_editorial_low_0  1.784248e-02\n",
            "6                            cant_votos_5  1.593923e-02\n",
            "11                          cant_votos_10  1.468184e-02\n",
            "60                      editorial_low_PC1  1.398134e-02\n",
            "22              id_lector_editorial_low_1  1.288742e-02\n",
            "61                 anio_edicion_rango_PC1  1.237480e-02\n",
            "18                  id_lector_generolit_1  1.234095e-02\n",
            "25         id_lector_anio_edicion_rango_0  1.215238e-02\n",
            "5                            cant_votos_4  1.143881e-02\n",
            "52                           Novela negra  1.079721e-02\n",
            "49                              Narrativa  1.078237e-02\n",
            "19                  id_lector_generolit_0  1.012549e-02\n",
            "39                      Ficción literaria  8.859072e-03\n",
            "57                         titulo_low_PC1  7.511383e-03\n",
            "42                  Histórica y aventuras  7.142236e-03\n",
            "47               Literatura contemporánea  6.453697e-03\n",
            "28                   Biografías, Memorias  6.342164e-03\n",
            "24         id_lector_anio_edicion_rango_1  6.180516e-03\n",
            "3                            cant_votos_2  5.697218e-03\n",
            "59                          autor_low_PC1  5.077934e-03\n",
            "4                            cant_votos_3  5.074541e-03\n",
            "2                            cant_votos_1  4.246098e-03\n",
            "44                     Infantil y juvenil  4.093582e-03\n",
            "37                     Estudios y ensayos  3.922673e-03\n",
            "31              Clásicos de la literatura  3.856839e-03\n",
            "38            Fantástica, ciencia ficción  3.704060e-03\n",
            "13                            generolit_0  3.654560e-03\n",
            "12                            generolit_1  3.614421e-03\n",
            "54                     Romántica, erótica  3.250648e-03\n",
            "58                         titulo_low_PC2  3.103475e-03\n",
            "51                             No ficción  2.731899e-03\n",
            "43                                  Humor  2.665289e-03\n",
            "0                              nacimiento  2.578285e-03\n",
            "46               Lecturas complementarias  2.493750e-03\n",
            "53                         Poesía, Teatro  2.327354e-03\n",
            "55                                 Varios  2.136918e-03\n",
            "33                 Cómics, Novela Gráfica  1.974636e-03\n",
            "56                  generolit_desconocido  1.298912e-03\n",
            "89                 generolit_Novela negra  9.579897e-04\n",
            "40                               Historia  8.725666e-04\n",
            "50                    Narrativa histórica  7.602169e-04\n",
            "62                           genero_Mujer  4.529331e-04\n",
            "27             Autoayuda Y Espiritualidad  4.515729e-04\n",
            "79        generolit_Histórica y aventuras  4.305182e-04\n",
            "86                    generolit_Narrativa  4.194971e-04\n",
            "26                                   Arte  4.152858e-04\n",
            "45                                Juvenil  3.586933e-04\n",
            "91           generolit_Romántica, erótica  3.392811e-04\n",
            "30                       Ciencias Humanas  2.436806e-04\n",
            "76            generolit_Ficción literaria  2.117303e-04\n",
            "36                                Empresa  2.043549e-04\n",
            "81           generolit_Infantil y juvenil  1.921156e-04\n",
            "75  generolit_Fantástica, ciencia ficción  1.892031e-04\n",
            "70       generolit_Cómics, Novela Gráfica  1.746641e-04\n",
            "35                               Economía  1.544057e-04\n",
            "84     generolit_Literatura contemporánea  1.410013e-04\n",
            "34                      Deportes Y Juegos  1.314240e-04\n",
            "48                                 Música  1.213791e-04\n",
            "41                      Historia del cine  9.912477e-05\n",
            "68    generolit_Clásicos de la literatura  9.396506e-05\n",
            "29                               Ciencias  5.981092e-05\n",
            "80                        generolit_Humor  2.875700e-05\n",
            "74           generolit_Estudios y ensayos  2.543997e-05\n",
            "93        generolit_generolit_desconocido  1.875012e-05\n",
            "65         generolit_Biografías, Memorias  1.479059e-05\n",
            "83     generolit_Lecturas complementarias  7.455843e-06\n",
            "32                                 Cocina  6.534484e-06\n",
            "90               generolit_Poesía, Teatro  6.093843e-06\n",
            "88                   generolit_No ficción  3.562126e-06\n",
            "87          generolit_Narrativa histórica  6.145527e-07\n",
            "92                       generolit_Varios  2.680862e-07\n",
            "85                       generolit_Música  0.000000e+00\n",
            "63                         generolit_Arte  0.000000e+00\n",
            "64   generolit_Autoayuda Y Espiritualidad  0.000000e+00\n",
            "73                      generolit_Empresa  0.000000e+00\n",
            "72                     generolit_Economía  0.000000e+00\n",
            "82                      generolit_Juvenil  0.000000e+00\n",
            "66                     generolit_Ciencias  0.000000e+00\n",
            "67             generolit_Ciencias Humanas  0.000000e+00\n",
            "71            generolit_Deportes Y Juegos  0.000000e+00\n",
            "78            generolit_Historia del cine  0.000000e+00\n",
            "77                     generolit_Historia  0.000000e+00\n",
            "69                       generolit_Cocina  0.000000e+00\n",
            "968\n"
          ]
        }
      ]
    }
  ]
}